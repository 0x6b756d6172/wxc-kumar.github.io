<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Flying a Racing Drone with EEG Mind Control | Journal</title>
    <meta name="description" content="This project log documents building a system to allow a user to fly a virtual racing drone using their thoughts via an EEG headset and a regression network trained on flight recordings.">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/assets/css/0.styles.119916c2.css" as="style"><link rel="preload" href="/assets/js/app.250ae4e3.js" as="script"><link rel="preload" href="/assets/js/3.12cdb4dc.js" as="script"><link rel="preload" href="/assets/js/5.97a377ba.js" as="script"><link rel="preload" href="/assets/js/7.68cbbc9e.js" as="script"><link rel="prefetch" href="/assets/js/10.5ae0cbec.js"><link rel="prefetch" href="/assets/js/11.616c26eb.js"><link rel="prefetch" href="/assets/js/12.3d11e13f.js"><link rel="prefetch" href="/assets/js/13.80fc7817.js"><link rel="prefetch" href="/assets/js/14.7f5980b6.js"><link rel="prefetch" href="/assets/js/15.99464e4f.js"><link rel="prefetch" href="/assets/js/16.127f6030.js"><link rel="prefetch" href="/assets/js/17.fa1b58d3.js"><link rel="prefetch" href="/assets/js/18.c46e3b3c.js"><link rel="prefetch" href="/assets/js/19.8b60c14a.js"><link rel="prefetch" href="/assets/js/2.3e04dc5f.js"><link rel="prefetch" href="/assets/js/20.803d20b1.js"><link rel="prefetch" href="/assets/js/21.5e245fa2.js"><link rel="prefetch" href="/assets/js/22.b168b163.js"><link rel="prefetch" href="/assets/js/23.7bd29250.js"><link rel="prefetch" href="/assets/js/4.b0ef2bca.js"><link rel="prefetch" href="/assets/js/6.23710a32.js"><link rel="prefetch" href="/assets/js/8.eb5991ba.js"><link rel="prefetch" href="/assets/js/9.9da839ba.js">
    <link rel="stylesheet" href="/assets/css/0.styles.119916c2.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Journal</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Flying a Racing Drone with EEG Mind Control</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/projects/control/#summary" class="sidebar-link">Summary</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/control/#current-status" class="sidebar-link">Current Status</a></li><li class="sidebar-sub-header"><a href="/projects/control/#future-work" class="sidebar-link">Future Work</a></li></ul></li><li><a href="/projects/control/#goal" class="sidebar-link">Goal</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/control/#theory" class="sidebar-link">Theory</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/control/#challenges" class="sidebar-link">Challenges</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/control/#feasibility" class="sidebar-link">Feasibility</a></li><li class="sidebar-sub-header"><a href="/projects/control/#technical" class="sidebar-link">Technical</a></li></ul></li><li><a href="/projects/control/#hardware-and-software-components" class="sidebar-link">Hardware and Software Components</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/control/#simulator-liftoff" class="sidebar-link">Simulator - Liftoff</a></li><li class="sidebar-sub-header"><a href="/projects/control/#eeg-openbci-ultracortex" class="sidebar-link">EEG - OpenBCI UltraCortex</a></li><li class="sidebar-sub-header"><a href="/projects/control/#controller-fatshark-controller" class="sidebar-link">Controller - FatShark Controller</a></li><li class="sidebar-sub-header"><a href="/projects/control/#software" class="sidebar-link">Software</a></li><li class="sidebar-sub-header"><a href="/projects/control/#compute" class="sidebar-link">Compute</a></li></ul></li><li><a href="/projects/control/#pipeline" class="sidebar-link">Pipeline</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/control/#recording-and-playback" class="sidebar-link">Recording and Playback</a></li><li class="sidebar-sub-header"><a href="/projects/control/#data" class="sidebar-link">Data</a></li><li class="sidebar-sub-header"><a href="/projects/control/#preprocessing" class="sidebar-link">Preprocessing</a></li><li class="sidebar-sub-header"><a href="/projects/control/#subsampling" class="sidebar-link">Subsampling</a></li><li class="sidebar-sub-header"><a href="/projects/control/#rolling-window" class="sidebar-link">Rolling Window</a></li><li class="sidebar-sub-header"><a href="/projects/control/#gramain-angular-field-transformation" class="sidebar-link">Gramain Angular Field Transformation</a></li></ul></li><li><a href="/projects/control/#networks" class="sidebar-link">Networks</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/control/#cnn" class="sidebar-link">CNN</a></li><li class="sidebar-sub-header"><a href="/projects/control/#cnn-with-skip" class="sidebar-link">CNN with Skip</a></li><li class="sidebar-sub-header"><a href="/projects/control/#fnn" class="sidebar-link">FNN</a></li><li class="sidebar-sub-header"><a href="/projects/control/#rcnn" class="sidebar-link">RCNN</a></li></ul></li><li><a href="/projects/control/#training" class="sidebar-link">Training</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/control/#future-work-2" class="sidebar-link">Future Work</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/control/#citations" class="sidebar-link">Citations</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="hero" data-v-7772bab1><h1 data-v-7772bab1>Flying a Racing Drone with EEG Mind Control</h1> <div class="image" style="background-image:url(/images/control-cover.png);" data-v-7772bab1></div> <p data-v-7772bab1>Published: 2019-02-10</p> <p data-v-7772bab1>  Updated: 2020-02-10</p></div> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <p>This project log documents building a system to allow a user to fly a virtual racing drone using their thoughts via an EEG headset. To accomplish this, a regression neural network is trained to take recordings of the user flying a drone using a flight controller while wearing an EEG headset.</p> <h3 id="current-status"><a href="#current-status" class="header-anchor">#</a> Current Status</h3> <p><strong>This project is still a work in progress</strong>. The end goal of flying by thought is has not yet been achieved. However, the full recording, training and playback pipeline is in place and the networks show that they are learning and even some simple responsiveness has been observed during network playback.</p> <h3 id="future-work"><a href="#future-work" class="header-anchor">#</a> Future Work</h3> <p>As noted above, while incomplete, the project is currently in a stable enough pipeline state, which means the whole pipeline can be tested and incrementally improved based on findings. Future work includes improving and decoupling parts of the recording pipeline, experimenting with network architectures and improving performance, especially the playback rate.</p> <h2 id="goal"><a href="#goal" class="header-anchor">#</a> Goal</h2> <p>The goal of this project is to build a system to allow users to fly a simulated drone via thought. This process is split into two steps of deep learning and deep reinforcement learning. The first step to build a regression network that is trained on recorded flights and can allow for basic control of drones. The second step is to take this trained network and set it as the starting point for a deep reinforcement learning algorithm where the user trains the algorithm actively/with online training.</p> <h2 id="theory"><a href="#theory" class="header-anchor">#</a> Theory</h2> <p>The central question is whether current prosumer technologies can be used to build a brain computer interface sensitive and responsive enough to perform a task as complicated as flying a high speed racing drone. While the fields of neuroscience and machine learning are immense, the project pulls from a handful of key pieces of research with the hopes that a network can be trained to handle the finer details.</p> <h2 id="challenges"><a href="#challenges" class="header-anchor">#</a> Challenges</h2> <p>There are many challenges when it comes to the topic of whether this project is feasible at all. The two that are most prevalent are the network training technique and the data acquisition methods.</p> <h3 id="feasibility"><a href="#feasibility" class="header-anchor">#</a> Feasibility</h3> <p>The core unanswered question of this project is whether it is possible at all to build such a system with the current hardware and software constraints.</p> <p>Firstly, EEG headsets only collect approximate data, even one as sensitive as OpenBCI. For example, an invasive electrode implanted in the brain can collect as fine of a reading as a single neuron because it is directly implanted onto the brain in direct contact with the neurons. An EEG headset however sits several centimeters away from it's source and collects the combined activation of thousands, possibly millions, of neurons. While this is useful for seeing the activity of for different regions of neurons (i.e. visual cortex lights up when it sees a red circle on the screen), it likely isn't enough to sense intent of drone flight at high speeds. There is research being done into potential solution to this issue that doesn't require invasive surgery. Some research has shown that it may be possible to bypass the brain and collect data from the spinal column. The spinal column and the lower motor neurons are the bodies natural outputs designed to generate signals meant to be translated into actions. Additionally, through methods like EMG, it is possible to get very close to the nerves and neurons via skin contact electrodes than is possible directly on the skull. Further investigation of this is required.</p> <p>Another issue is that thinking about performing an action produces different brain activity than actually performing that action (and consequentially, thinking about thinking of an action yet again produces different activity). Currently these networks are being trained currently on recordings of a user flying a drone with a controller in which they are producing real physical actuation to control the drone. However, to test the trained networks, the user has to attempt to fly it with only their thoughts. It is currently unclear whether this disparity can be overcome by the current training pipeline (or possibly for that matter, any pipeline where the training and test environments differ in this way). Overcoming this challenge remains an open question but one consideration is to bring in reinforcement learning earlier into the process. The key challenge there is that reinforcement learning has the cold start issue - as many as the first several hundred iterations can be random while the algorithm learns the basics of the environment. No human can sit through that training process of random activity.</p> <h3 id="technical"><a href="#technical" class="header-anchor">#</a> Technical</h3> <p><strong>Data Collection</strong>: This project generates data from several locations, including the EEG headset, controller and screen. This pieces of information are highly correlated - when a wall comes up on the screen, the user uses see the information on the screen and the EEG headset receives neural activity from the brain. These signals come from different devices generated in a unsynchronized streams that must be lined up and recorded together to represent a sample. In Python, the Global Interpreter Lock prevents the a program from achieving in-process parallelization and requires <code>multiprocessing</code> which has added overhead and code complexity. Currently <code>PyLSL</code> is being used to handle this task but is not meant to act as a production library. Further works remains in this area.</p> <p><strong>Responsiveness</strong>: racing drones fly at incredible speeds; for the purposes of this project the simulated drone was adjusted to allow it to fly at a slower speed but in many cases the drone still topped out at 60+ KPH. In addition to this, drones are incredibly sensitive to input. Most drones have software to dampen and scale input from the user before it even gets to the motors. This is used by professional drone racings to do amazing maneuvers, turns, flips and tricks. However, this same responsiveness is a challenge for flight by mind for two reasons: first, if the rate of predictions is not high enough then the network will not be making predictions fast enough to respond to the state of the drone. If a drone is approaching a wall at high speed but predictions only happen once or twice a second, it will not have enough time to react, even if the predictions are perfect. Secondly, predictions must not have a lot of jitter. If predictions vary wildly from prediction to prediction, then the drone will also respond unpredictability and possibly cause a negative feedback loop compounding errors. Combined, this means that predictions need to be made &quot;realtime&quot; (the minimum threshold for this project has been decided as 30 Hz, though 60 Hz is the ideal) but also have very little jitter.</p> <h2 id="hardware-and-software-components"><a href="#hardware-and-software-components" class="header-anchor">#</a> Hardware and Software Components</h2> <p>The following section discusses the preexisting components of the pipeline that were purchased or otherwise integrated in to the design.</p> <p><img src="/assets/img/gear.6e09bae1.png" alt=""></p> <h3 id="simulator-liftoff"><a href="#simulator-liftoff" class="header-anchor">#</a> Simulator - Liftoff</h3> <p>The first step in the process was to pick a drone simulator. The two key deciding factors were the quality of simulation, including physics but also other aspects such as virtual drone configuration, flight controller settings, etc. and Linux support. <a href="https://store.steampowered.com/app/410340/Liftoff_FPV_Drone_Racing/" target="_blank" rel="noopener noreferrer">Liftoff<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> was chosen for this project as it met these requirements. The Vortex Pro 250 was chosen as the drone for all recordings as it is a common frame for racing drones among beginners and offers a good balance of speed and control. The stock drone was unmodified expect for changing the camera angle to 20 degrees to allow for slower flight while keeping the horizon in view and setting the Flight Controller to &quot;Freestyle Smooth&quot;. Additionally, the game options were set to the maximum settings (minus Bloom and Shadows, which caused stutter within Liftoff) with a framerate limit of 60fps. Finally, the On Screen Display (OSD) was enabled, which shows the horizon line, speed and altitude, which is expected to give some additional information about the orientation and the world to the model.</p> <h3 id="eeg-openbci-ultracortex"><a href="#eeg-openbci-ultracortex" class="header-anchor">#</a> EEG - OpenBCI UltraCortex</h3> <p>The OpenBCI <a href="https://shop.openbci.com/collections/frontpage/products/ultracortex-mark-iv" target="_blank" rel="noopener noreferrer">UltraCortex Mark IV<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> with 8 channel <a href="https://shop.openbci.com/collections/frontpage/products/cyton-biosensing-board-8-channel?variant=38958638542" target="_blank" rel="noopener noreferrer">Cyton<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> board and <a href="https://shop.openbci.com/collections/frontpage/products/5-mm-spike-electrode-pack-of-30?variant=8120433606670" target="_blank" rel="noopener noreferrer">Dry Comb Electrodes<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> was chosen for its high quality sensing capabilities as well as for it's (relatively) low cost.</p> <h3 id="controller-fatshark-controller"><a href="#controller-fatshark-controller" class="header-anchor">#</a> Controller - FatShark Controller</h3> <p>In order to fly the simulated drone accurately and to train the network on realistic output, the <a href="https://www.amazon.com/gp/product/B079C52D69/" target="_blank" rel="noopener noreferrer">FatShark Controller<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> was chosen as a low cost option for a real drone controller with USB joystick interface. The FatShark controller work natively with both Liftoff and Linux and was interfaced with <code>python-evdev</code>.</p> <h3 id="software"><a href="#software" class="header-anchor">#</a> Software</h3> <p>A variety of libraries were used to bring this project together. These are some of the more notable ones, whose usage is described below.</p> <ul><li>Deep Learning: FastAI and PyTorch</li> <li>EEG Encoding: PyTS (Gramian Angular Fields)</li> <li>Screen Capture: MSS</li> <li>Data Handling: PyLSL, PyLMDB, Pandas, Numpy</li> <li>Controller Capture and Emulation: python-evdev</li></ul> <h3 id="compute"><a href="#compute" class="header-anchor">#</a> Compute</h3> <ul><li>Recording Rig: MSI P65 Creator with NVIDIA GTX 1060 MQ and screen resolution set to 720p</li> <li>Training Rig: Self built rig, VM with PCI passthrough with NVIDIA GTX 2070 with 8GB of RAM, Intel CPU with 6 Cores and 24 gigs of RAM.</li></ul> <h2 id="pipeline"><a href="#pipeline" class="header-anchor">#</a> Pipeline</h2> <p>The pipeline consists of three components, the hardware which includes EEG headset, controller and laptop running the simulator, the Python application responsible for data collection and model playback and the data processing and training done in Jupyter notebooks.</p> <p><img src="/assets/img/control.04a0ab40.png" alt=""></p> <h3 id="recording-and-playback"><a href="#recording-and-playback" class="header-anchor">#</a> Recording and Playback</h3> <p>The recording of data and playback of trained models is done with a Python application. Python was chosen because of ease of importing a PyTorch model trained in Python, but Python is not the ideal choice for such a tool due to the high frequency requirements. A single process is unable to screen capture, save the image, get EEG data from OpenBCI, get input state from controller and also make a prediction at anything more than 5 Hz.</p> <p>As such, the application had to be designed around <code>multiprocessing</code> and <code>pyLSL</code>. Multiprocessing was used to split up each component into it's own process, with LSL as the communication layer between them. Before LSL was introduced, multiprocessing constructs such as <code>multiprocessing.Value</code>,  <code>multiprocessing.Array</code> and <code>multiprocessing.Queue</code> were first attempted but they each had their own issues and were ultimate too slow due to lock contention between the processes running at different frequencies. Additionally, LSL provides a timestamp that can be used to synchronize disjointed data.</p> <p>This final architecture allowed BCI and Controller data to be sampled at 240 Hz and screen frames to be sampled at 720p@30 Hz (it was not possible to push screen capture beyond this via MSS and Python and maintain a steady frame rate). Playback however is limited to a max of 40 Hz, which is mainly due to the amount of time it takes to make a prediction - GPU predictions are significantly faster than CPU predictions, however, the simulator also uses the GPU so while Playback is running. Because both network predictions and game rendering are happening on the GPU, this ultimately limits the frequency to 40 Hz with screen image or about 60 Hz without.</p> <h4 id="controller-sampler-emulator"><a href="#controller-sampler-emulator" class="header-anchor">#</a> Controller Sampler/Emulator</h4> <p>The controller process has two functions: 1) to read events from the controller via <code>evdev</code>, storing those events as controller state and 2) replicating those events an emulated controller via <code>uinput</code>, both of which are done using the <a href="https://python-evdev.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">python-evdev<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> library. This emulation is done so that during playback, the controller can be driven by the predictions of the network and passed along to the game without the real controller. The FatShark controller produces input on all axis between 21 and 232, so all input is scaled between 0 and 1 when recorded and scaled back by multiplying against 211 (232 - 21) when being played back. This creates a smooth value, for example where 50% represents moving the stick halfway.</p> <h4 id="eeg-sampler"><a href="#eeg-sampler" class="header-anchor">#</a> EEG Sampler</h4> <p>EEG sampling is done via OpenBCI's Python library <a href="https://github.com/OpenBCI/OpenBCI_Python" target="_blank" rel="noopener noreferrer">openbci-python<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. The usage is simply as the example provided by OpenBCI, by calling <code>board.start_stream()</code>. Data read is placed into an LSL stream, which both the recording and playback functionality read from.</p> <h4 id="screen-sampler"><a href="#screen-sampler" class="header-anchor">#</a> Screen Sampler</h4> <p>Screen sampling is done via the <a href="https://python-mss.readthedocs.io" target="_blank" rel="noopener noreferrer">MSS<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> library which handles efficiently taking images of the screen. The screen is captured at 1280x720p, converted from MSS's format to PIL using <code>Image.frombytes</code> then downscaled to 100x100 using <code>PIL.Image.LANCZOS</code> (it was noted that <code>PIL.Image.NEAREST</code> caused too much degredation of the image). Even with it's size reduced, at full speed this process generates a lot of data which makes it difficult to make it available to the recorder process in time. Writing the image to disk is one option, however PNGs take too long to write resulting in a lower frame rate and while JPGs can be written out quickly, they had a noticeable loss in quality and artifacting. Instead, the image is saved in byte form using <code>pil.tobytes()</code> into an <a href="https://lmdb.readthedocs.io/en/release/" target="_blank" rel="noopener noreferrer">LMDB<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> database. Because LMDB is memory mapped, it bypasses a lot of the work involved in saving something to disk. The key of the image is set as the timestamp which is provided by LSL. Later, before training, all images are extracted from the LMDB database, converted to PNGs and saved to disk using the timestamp as it's filename, where the training script loads it from during training time. This process results in an approximate top FPS of 50 Hz +- 15 Hz. In order to maintain a steady frame rate, which is necessary for the screen capture process is locked to a maximum of 30 Hz.</p> <h4 id="recorder"><a href="#recorder" class="header-anchor">#</a> Recorder</h4> <p>The recorder is the simplest piece of the process, which simply gets the values from each of the other processes via pyLSL and saves them to a CSV file.</p> <h4 id="playback"><a href="#playback" class="header-anchor">#</a> Playback</h4> <p>Finally, the final piece of tool is the playback functionality, which loads in a trained model, then in a loop reads EEG data and screen information, prepares matching the data preprocessing described below and feeds it into the network to make a prediction. The prediction is then injected into the Input LSL stream, which then controls the virtual controller, flying the drone.</p> <h3 id="data"><a href="#data" class="header-anchor">#</a> Data</h3> <p>The final output from the recording tool is a series of CSVs, one for each run of 30 seconds. Each row contains values for each of the eight channels of EEG data, four values for the recorded controller state (x and y axis for two joysticks) and a timestamp of the screenshot, as well as two additional values for differentiating recordings and runs. An example of the first five values of a CSV is shown below.
<img src="/assets/img/data.41212a8a.png" alt=""></p> <h3 id="preprocessing"><a href="#preprocessing" class="header-anchor">#</a> Preprocessing</h3> <p>No explicit preprocessing is done on the data. Whereas most papers typically perform at least one data preprocessing step, such as a high pass filter, etc., this project hands of all such processing to the network and expects is to learn those steps. There is some work demonstrating that this method produces better results that traditional preprocessing. <strong>TODO: cite relevant stuff here</strong></p> <h3 id="subsampling"><a href="#subsampling" class="header-anchor">#</a> Subsampling</h3> <p>While the EEG data is polled at the max frequency of 250 Hz from OpenBCI hardware and API, the recorder saves the data at a slightly lower rate of 240 Hz. This allows for subsampling which is a common data augmentation technique as described in <code>TODO: CITE SOURCE</code>. Subsampling was experimented with at various gaming framerates, such as 30 Hz, 60 Hz and 120 Hz by simply using Python slicing notiation (e.g. <code>data[::4]</code> for 60 Hz). The remaining data is kept as another &quot;recording&quot;, allowing for the full usage of the recorded data.</p> <p><img src="/assets/img/subsampling.f5d985ad.png" alt=""></p> <h3 id="rolling-window"><a href="#rolling-window" class="header-anchor">#</a> Rolling Window</h3> <p>A single sample is not enough for a network to make a prediction in a time series environment. As such a sample that is fed into the network is actually a number of sequential samples put together to form a timestep. This number is determined as a percentage of 1 second - i.e. at 60 Hz and 0.25 second size sample will have 15 concurrent timesteps in it. The value 0.25 seconds per training sample was chosen as it is a commonly accepted value of human visual reaction time [CITATION NEEDED] though further experimentation is needed as the subsampled rate decreases, so do the number of samples per timestep. Currently, the overlap window is set to one and further testing of the overlap is required, though there is some evidence to suggest that the highest level of overlap produces better results [CITATION NEEDED]. Because each timestep needs n samples, each run is padded with n - 1 zeros so that Timestep 1 enough samples available to it.</p> <p><img src="/assets/img/windowing.06b38b7c.png" alt=""></p> <h3 id="gramain-angular-field-transformation"><a href="#gramain-angular-field-transformation" class="header-anchor">#</a> Gramain Angular Field Transformation</h3> <p>Input data is all data that is used to make the final prediction. Currently, this includes the timestep, 8 channels of EEG data and the controller input from the previous timestep but could be anytime series data that is relevant to the prediction (for example, eyetracking coordinates or gyro data available via the OpenBCI headset). The depending on the network the may be transformed. In the case of an FNN, the data is not transformed but fed into the network directly. However, it is necessary to transform the series of data into a format that CNN based networks can understand.</p> <p>There are many options available for this step but Gramian Angular Fields were chosen for this task, as presented in [CITE PAPER] and implemented by <a href="https://pyts.readthedocs.io/en/latest/auto_examples/image/plot_gaf.html" target="_blank" rel="noopener noreferrer">PyTS<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Gramain Angular Fields have shown success with CNNs beyond its original paper and are further discussed <a href="https://rf5.github.io/2019/04/20/xrd-fastai.html" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and <a href="https://medium.com/analytics-vidhya/encoding-time-series-as-images-b043becbdbf3" target="_blank" rel="noopener noreferrer">here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. Each column (channel) of data is transformed into a single image, then stacked together to form the input for a CNN. The resulting image is 9 channels so the receiving network must be appropriately adjusted.</p> <p>The following image shows a GADF transformed timestep with a 0.25 second window of 60 Hz data from a random point with the Virdis colormap. The first image is the timestamp of this timestep and the remaining 8 are channels 1-8 of EEG data.</p> <p><img src="/assets/img/gadf.597d049f.png" alt=""></p> <h2 id="networks"><a href="#networks" class="header-anchor">#</a> Networks</h2> <p>The question of ideal network architecture remains an open question. Several types of networks are being experimented with, including FNNs, CNNs, RNNs and RCNNs, with a variety of hyperparameters. The two most promising network designs are likely to be CNNs or RCNNs, which have shown a great deal of success in the metastudy [CITATION]. One item that all networks have in common is the CNN architecture. Every network that has a CNN is using a the FastAI implemenation of a ResNet modified as described in the Bag Tricks paper called the <a href="https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py" target="_blank" rel="noopener noreferrer">XResNet<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>. These networks were tested with both the pretrained ResNet from PyTorch as well as the XResNet and the XResNet tended to perform better in every case. This is likely because the encoded date isn't represented in the pretrained ResNet which is trained to look for features for real things as opposed to abstract relationships in images like a GADF transformed data series.</p> <h3 id="cnn"><a href="#cnn" class="header-anchor">#</a> CNN</h3> <p>The CNN architecture is composed of three XResNet18s, a concat layer and two fully connected layers with batchnorm and dropout. The three XResNet's correspond to the three pieces of input currently being made available to the network: a multichannel GADF transformed image of the input data, a multichannel frameskipped image of the frames and another multichannel GADF transformed image of the input in prior timesteps.
<img src="/assets/img/cnn.45685417.png" alt=""></p> <h3 id="cnn-with-skip"><a href="#cnn-with-skip" class="header-anchor">#</a> CNN with Skip</h3> <p>This architecture is similar to the plain CNN above, however, it adds a skip connection (similar to the residual block of a ResNet) to preserve the context of data after the ResNet step. Testing shows that this method out performs the plain CNN every time at the cost of added complexity (and runtime).
<img src="/assets/img/cnn-skip.bfd9f5d8.png" alt=""></p> <h3 id="fnn"><a href="#fnn" class="header-anchor">#</a> FNN</h3> <p>Because the above CNN with Skip network performed so well, this network does away with the GADF transformation and the associated ResNets, bringing the ResNet count down to one, for the frames stack image. The resulting network out performs the plain CNN network and ties with the CNN with Skip, suggesting that the additional ResNets may not be necessary.
<img src="/assets/img/fnn.17ff0706.png" alt=""></p> <h3 id="rcnn"><a href="#rcnn" class="header-anchor">#</a> RCNN</h3> <p>The RCNN is a hybrid of CNN and RNN networks, which [CITE PAPER] shows is growing in popularity and effectiveness. The network takes two XResnets, one for the values and the other for the frames. In this scenario, the previous input is skipped, as this is one of the things the network is expected to and capable of learning due to have an RNN. The outputs from the XResNets are concatenated together and then fed into two GRU layers and then two linear layers. The current design of the network uses two GRU layers. Future considerations include using an AWD-LSTM as well as QRNNs.
<img src="/assets/img/rcnn.068487cd.png" alt=""></p> <h2 id="training"><a href="#training" class="header-anchor">#</a> Training</h2> <p>Training is done using FastAI's training loop, which among other things, uses Adam with One Cycle training. No changes or modifications were made to the training process itself - most testing is being done at the recommended <code>3e-3</code>. Changes in learning rate, including with recommendations from FastAI LR Finder did not result in any substantial improvements and in some cases the recommended values resulted in worse performance.</p> <h2 id="future-work-2"><a href="#future-work-2" class="header-anchor">#</a> Future Work</h2> <ul><li>improving data recording stability. The data recording tends to be brittle, especially around OpenBCI, which appears to have random dropped packets and disconnects</li> <li>refining the &quot;chosen&quot; network architecture. Currently, several network architectures are being experimented with, including FNNs, CNNs, RNNs, and RCNNs. Some of these perform significantly better than others during training time, however none yet have achieved performance good enough to be considered the best design.</li> <li>taking further neuroscience research into considering</li> <li>incorporating reinforcement learning (likely the Actor-Critic model)</li></ul> <h2 id="citations"><a href="#citations" class="header-anchor">#</a> Citations</h2> <p>[1] Roy, Y., Banville, H., Albuquerque, I., Gramfort, A., Falk, T. H., &amp; Faubert, J. (2019, August 14). Deep learning-based electroencephalography analysis: a systematic review. Journal of Neural Engineering. Retrieved from https://iopscience.iop.org/article/10.1088/1741-2552/ab260c</p> <p>[2] Basmajian, J. V. (1963). Control and Training of Individual Motor Units. Science, 141(3579), 440–441. doi: 10.1126/science.141.3579.440. Retrieved from https://science.sciencemag.org/content/141/3579/440</p> <p>[3] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602. Retrieved from https://arxiv.org/abs/1312.5602</p> <p>[4] Wang, Z., &amp; Oates, T. (2015, June). Imaging time-series to improve classification and imputation. In Twenty-Fourth International Joint Conference on Artificial Intelligence. Retrieved from https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/11082</p> <p>[5] He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., &amp; Li, M. (2019). Bag of tricks for image classification with convolutional neural networks. Retrieved from https://arxiv.org/abs/1607.01759</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.250ae4e3.js" defer></script><script src="/assets/js/3.12cdb4dc.js" defer></script><script src="/assets/js/5.97a377ba.js" defer></script><script src="/assets/js/7.68cbbc9e.js" defer></script>
  </body>
</html>
