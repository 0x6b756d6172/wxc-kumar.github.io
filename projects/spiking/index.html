<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Spiking Neural Networks in Elixir | Journal</title>
    <meta name="description" content="Project log for building a spiking neural network with Elixir, BEAM, PubSub and LiveView">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/assets/css/0.styles.1f1142dd.css" as="style"><link rel="preload" href="/assets/js/app.54e24dc4.js" as="script"><link rel="preload" href="/assets/js/3.12cdb4dc.js" as="script"><link rel="preload" href="/assets/js/20.db6df5b7.js" as="script"><link rel="preload" href="/assets/js/7.8686dcb9.js" as="script"><link rel="prefetch" href="/assets/js/10.c810b518.js"><link rel="prefetch" href="/assets/js/11.616c26eb.js"><link rel="prefetch" href="/assets/js/12.3d11e13f.js"><link rel="prefetch" href="/assets/js/13.80fc7817.js"><link rel="prefetch" href="/assets/js/14.7f5980b6.js"><link rel="prefetch" href="/assets/js/15.81df7efb.js"><link rel="prefetch" href="/assets/js/16.dc7eb4d8.js"><link rel="prefetch" href="/assets/js/17.fa1b58d3.js"><link rel="prefetch" href="/assets/js/18.1a79fdf5.js"><link rel="prefetch" href="/assets/js/19.deb3f6ba.js"><link rel="prefetch" href="/assets/js/2.3e04dc5f.js"><link rel="prefetch" href="/assets/js/21.5e245fa2.js"><link rel="prefetch" href="/assets/js/22.b168b163.js"><link rel="prefetch" href="/assets/js/23.7bd29250.js"><link rel="prefetch" href="/assets/js/4.98e26cc5.js"><link rel="prefetch" href="/assets/js/5.163e8f83.js"><link rel="prefetch" href="/assets/js/6.a4957363.js"><link rel="prefetch" href="/assets/js/8.4305e23b.js"><link rel="prefetch" href="/assets/js/9.9da839ba.js">
    <link rel="stylesheet" href="/assets/css/0.styles.1f1142dd.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Journal</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Spiking Neural Networks in Elixir</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/projects/spiking/#introdution" class="sidebar-link">Introdution</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#erlang-beam-and-elixir" class="sidebar-link">Erlang, BEAM and Elixir</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#the-process" class="sidebar-link">The Process</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#the-scheduler" class="sidebar-link">The Scheduler</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#the-neuron" class="sidebar-link">The Neuron</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#liveview" class="sidebar-link">LiveView</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#neat-neuroevolution-of-augmenting-topologies" class="sidebar-link">NEAT (NeuroEvolution of Augmenting Topologies)</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#design-and-implementation" class="sidebar-link">Design and Implementation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/spiking/#mark-i-agents-genserver-and-supervisors" class="sidebar-link">Mark I: Agents, GenServer and Supervisors</a></li><li class="sidebar-sub-header"><a href="/projects/spiking/#mark-ii-genserver-and-phoenix-pubsub" class="sidebar-link">Mark II: GenServer and Phoenix PubSub</a></li></ul></li><li><a href="/projects/spiking/#blocking-issue-interfacing" class="sidebar-link">Blocking Issue: Interfacing</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/spiking/#mark-iii-research" class="sidebar-link">Mark III Research</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/spiking/#erlang-port-protocol" class="sidebar-link">Erlang Port Protocol</a></li><li class="sidebar-sub-header"><a href="/projects/spiking/#phoenix-webserver" class="sidebar-link">Phoenix Webserver</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="hero" data-v-7772bab1><h1 data-v-7772bab1>Spiking Neural Networks in Elixir</h1> <div class="image" style="background-image:url(/images/spiking-cover.jpg);" data-v-7772bab1></div> <p data-v-7772bab1>Published: 2020-02-10</p> <p data-v-7772bab1>  Updated: 2020-02-17</p></div> <h2 id="introdution"><a href="#introdution" class="header-anchor">#</a> Introdution</h2> <p>The technology that is powering todays artificial intelligence boom was inspired by biology. Artificial Neural Networks (ANNs) were originally designed to mimic biological neurons, including how they connect to each other and how they communicate with each other. While it's conception was inspired by biology, ANN's took a sharp turn away from biological models fairly quickly. Deep Learning as it is known today still uses the concept of neural networks but only in a very loose sense. Deep Neural Networks are &quot;layers&quot; of neurons connected to the neurons in the next layer and processed one layer at a time. This creates a rigid structure for forward only neuron connections. Additionally, while biological neurons work via action potential spikes, DNN's have mostly done away with that concept because activation such as ReLU have been found to work significantly better in DNN environments. Additionally, ANN's don't represent true neurons structures in the brain, which have different shapes, such as rows of pillars or a shell of more sophisticated neurons encapsulating less complex neurons.</p> <p>Deep Learning is effective, there is no doubt about that. Just like how planes were inspired by birds but don't flap their wings, neural networks don't need to emulate real neurons to perfection to work. However, there is a growing consensus in the AI community that deep learning has limitations. From the need for massive amounts of data to hours, days or months of training time for tasks that humans are able to learn in minutes. New machine learning techniques, such as reinforcement learning rely on deep learning as it's foundation for the next step of machine learning. In fact, one popular algorithm, known as Deep Q Learning is just this - the &quot;deep&quot; refers to a neural network that is used to approximate a value function that then the rest of the Q Learning algorithm uses to do things more sophisticated than a neural network itself could achieve.</p> <p>While our planes don't flap their wings, neurons and brains stand out as unique. Birds fly through wing flapping but paper planes and gliders don't. Unlike flight, brains are unique - there is no other known system that function like a brain. In fact, many scientists and engineers consider the brain to be the most complex and sophisticated machine in the universe - the only machine that can perceive, act and question it's own existence. This project is an attempt to simulate neurons clusters and structures realistically using a &quot;realtime&quot; environment. Elixir and BEAM have some key features that make this exploration possible. There isn't a specific goal for this project - that is, there is no final product or deliverable. Rather the objective is to simulate neurons to understand what a closer emulation can teach us about neuron interaction, intelligence and learning and to carry that research back into the real world of machine learning.</p> <h2 id="erlang-beam-and-elixir"><a href="#erlang-beam-and-elixir" class="header-anchor">#</a> Erlang, BEAM and Elixir</h2> <p>Erlang first appears in 1986, making it an old language by today's standard. However, with age comes wisdom - Erlang has been used in telecommunications for 30+ years and in that time, the engineers working on it have refined Erlang for high concurrency and fault tolerance. Similar to Java and the JVM or Python and the Python Interpreter, Erlang is the language that runs on the BEAM VM (Björn's Erlang Abstract Machine).</p> <p>Elixir is a new language, first appearing in 2011. Elixir runs on the BEAM VM, similar to Clojure, Kotlin or Scala for the JVM. Elixir and Erlang both produces artifcats in Erlang Abstract Format, which are fed into the BEAM VM. However, Elixir has the benefit of hindsight and is able to take into account 30 years of language design lessons, including language structure, concepts and tools. Consequentially, Elixir is much easier to use and provides the programmer with a an immense amount of power by allowing them to tap into the 30 years of concurrent distributed architecture design of BEAM, in addition to the constructs and libraries already available in Erlang.</p> <h2 id="the-process"><a href="#the-process" class="header-anchor">#</a> The Process</h2> <p>BEAM has only a handful of key constructs: Atoms (global enums), Maps (dictionaries), Enumerable (lists), Functions and the most interesting and relevant to this project: Processes. These processes are not OS level processes - rather, a BEAM VM process is a data structure of a few MBs that represent a process, it's state and it's mailbox. From the <a href="https://elixir-lang.org/" target="_blank" rel="noopener noreferrer">Elixir<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> website:</p> <blockquote><p>All Elixir code runs inside lightweight threads of execution (called processes) that are isolated and exchange information via messages. Due to their lightweight nature, it is not uncommon to have hundreds of thousands of processes running concurrently in the same machine. Isolation allows processes to be garbage collected independently, reducing system-wide pauses, and using all machine resources as efficiently as possible (vertical scaling). Processes are also able to communicate with other processes running on different machines in the same network. This provides the foundation for distribution, allowing developers to coordinate work across multiple nodes (horizontal scaling).</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>current_process = self()

# Spawn an Elixir process (not an operating system one!)
spawn_link(fn -&gt;
  send(current_process, {:msg, &quot;hello world&quot;})
end)

# Block until the message is received
receive do
  {:msg, contents} -&gt; IO.puts(contents)
end
</code></pre></div><h2 id="the-scheduler"><a href="#the-scheduler" class="header-anchor">#</a> The Scheduler</h2> <p>These processes require a scheduler to operate. The scheduler is responsible for ensuring that every process pulls from it's mailbox, performs it's action and then goes back to polling the mailbox. This is a step. The interesting feature of the scheduler is that every process in the VM gets one step before a process is allowed to step again. This means that while they are not actually &quot;realtime&quot; every process operates in lock step with every other process, effectively appearing to function in real time to <em>each other</em>, giving us the foundational block of the simulation. [CITATION REQUIRED]</p> <div class="language- extra-class"><pre class="language-text"><code>#Step Start
Process 1 Step 1
Process 2 Step 1
Process 3 Step 1
#Step End

#Step Start
Process 1 Step 2
Process 2 Step 2
Process 3 Step 2
#Step End
</code></pre></div><h2 id="the-neuron"><a href="#the-neuron" class="header-anchor">#</a> The Neuron</h2> <p>BEAM processes form the basis of neurons in the simulation. Each neuron is represented as a process, which receives inputs from other neurons as messages in it's mailbox and transmits activations to other neurons via messages. Connections can be formed in multiple ways, ranging from keeping a reference to the target neuron to using more sophisticated messaging methods discussed below.</p> <h2 id="liveview"><a href="#liveview" class="header-anchor">#</a> LiveView</h2> <p>Currently in prerelease state, Phoenix LiveView is a library for server side HTML rendering in Elixir. LiveView creates a socket connection to the DOM of the browser and automatically updates the view when the backing data is changed. In this way, using LiveView with a library like D3JS is an excellent choice for rendering of the networks for visual inspection and real time modification.</p> <h2 id="neat-neuroevolution-of-augmenting-topologies"><a href="#neat-neuroevolution-of-augmenting-topologies" class="header-anchor">#</a> NEAT (NeuroEvolution of Augmenting Topologies)</h2> <p>The paper Evolving Neural Networks Through Augmenting Topologies introduced the NEAT algorithm. NEAT is an evolutionary algorithm for not only training neural networks but for also building their structure through genetic algorithms. While this is not the only algorithm for neuroevolution, it is uniquely suited to this project because of the flexibility of network structures that can be generated - neurons that connect backwards, neurons that connect to form a loop, neurons that bypass huge chunks of the network, etc. At presented, NEAT is the algorithm being primarily considered for creating networks.</p> <h2 id="design-and-implementation"><a href="#design-and-implementation" class="header-anchor">#</a> Design and Implementation</h2> <h3 id="mark-i-agents-genserver-and-supervisors"><a href="#mark-i-agents-genserver-and-supervisors" class="header-anchor">#</a> Mark I: Agents, GenServer and Supervisors</h3> <p>The first implementation of this project used the Elixir construct Agents to represent a neuron. A network was an Agent with a collection of nueron agents and a cluster was an Agent with a collection of network agents. In addition, BEAM Supervisors were used to connect neurons, networks and clusters. Supervisors are a special type of process that watch other processes for crashes. Supervisors can respond to crashes by spining up new process, logging and load balancing and is an essential construct for high performance server side computing - Elixir's motto in fact is to &quot;Let it Crash&quot;, relying on BEAMs fault tolerance rather than complex coding.</p> <p>Unfortunately, this design became unweildy quickly. Supervisors don't have state where information such as the ID's of every neuron used by the network can be stored so in actuality, there were two processes for networks, one for the supervisor and one for the actor. Additionally, neurons had to store references to each others process IDs, however, process ID's are not allocated until after a neuron has started, adding complexity to initialization and modification. Finally, while supervisors are essential for server side processes such as data process, mail sending, login, etc. they are less essetential for a simulated neural network. Each neuron does very little work: read received input, add to current state, if current state is greater than activation threshold then transmit state to connected neurons. There are no other dependencies and if a single neuron fails in a network, then the whole network becomes invalidated. Ultimately, Mark I was too complex for the task at hand and shelved.</p> <h3 id="mark-ii-genserver-and-phoenix-pubsub"><a href="#mark-ii-genserver-and-phoenix-pubsub" class="header-anchor">#</a> Mark II: GenServer and Phoenix PubSub</h3> <p>The goal of Mark II was to simplify the neuron, network and cluster representation, creation and modification process. The first step was to eliminate the usage of Agents and fallback to GenServers. Agents are in fact a subclass of GenServers (Generic Servers). Agents are designed to store and retrieved data with logic (i.e a stack implementation around a list). GenServers on the other hand are functions that take in the state of a process and can optionally modify it, resulting in a simpler and direct interface to the data. Supervisors were also discarded at the neuron/network level as they added no additional value at this state, though there is a potential for reintroduction at a later stage as needed.</p> <p>Finally, the largest improvement made was to integrate Phoenix PubSub. PubSub is a standalone feature of the Phoenix library. PubSub allows processes to register to topics and all registered process receive any messages sent to that topic. Here, each neuron has an ID similar to <code>neuron:1:2:3</code> representing Cluster ID 1, Network ID 2, Neuron ID 3. All processes that &quot;connect&quot; to this neuron subscribe to the topic ID of the target neuron. When the subscribed neuron has an activation, it sends the activation to the topic and all connected neurons receive that message.</p> <h2 id="blocking-issue-interfacing"><a href="#blocking-issue-interfacing" class="header-anchor">#</a> Blocking Issue: Interfacing</h2> <p>After completing the Mark II revisions, a new roadblock was reached. It turns out that building such a network is the easy part of this problem. However, unlike the traditional machine learning language of choice, Python, Elixir is not designed for data science. While it's easy to build a network in Elixir, it's much harder to evaluate and test that network. Most of the required libraries in data science and machine learning such as PyTorch, Pandas, Numpy, OpenAI Gym, etc. are Python based. Rewriting such core functionality is simply too much overheard to maintain for an experimental design that changes often.</p> <h2 id="mark-iii-research"><a href="#mark-iii-research" class="header-anchor">#</a> Mark III Research</h2> <p>Instead of trying to reimplement data science in Elixir the current direction is to develop the neural simulation engine in Elixir and interface to it from outside using Python. The goal is to build the system that allows for the creation of networks, neurons and clusters via an API which can be queried and modified from outside of Elixir and BEAM, via Python. In this way, the Python libraries can be used to setup experiments (for example using OpenAI Gym) and connect to the simulated network in Elixir for predictions. Two options are being considered for this:</p> <h3 id="erlang-port-protocol"><a href="#erlang-port-protocol" class="header-anchor">#</a> Erlang Port Protocol</h3> <p>Erlang Port Protocol is the BEAM protocol for interfacing functions inside BEAM to the outside world. Similar to Python bindings for C libraries, using EPP, it is possible to talk directly to the functions inside Elixir to manipulate data. More about this can be found here: https://hackernoon.com/mixing-python-with-elixir-7a2b7ac6696</p> <h3 id="phoenix-webserver"><a href="#phoenix-webserver" class="header-anchor">#</a> Phoenix Webserver</h3> <p>Another alternative is to build a server with an API for performing specific tasks (i.e. create network with configuration x, delete neuron y, etc.). The Pheonix library has many communication methods ranging from REST endpoints to WebSockets. This approach unlike the EPP above would result in a more decoupled library which can be used with any language at the cost of added complexity and implementation work.</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.54e24dc4.js" defer></script><script src="/assets/js/3.12cdb4dc.js" defer></script><script src="/assets/js/20.db6df5b7.js" defer></script><script src="/assets/js/7.8686dcb9.js" defer></script>
  </body>
</html>
