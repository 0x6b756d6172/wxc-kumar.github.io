<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Webcam Eye Tracking with Neural Networks | Journal</title>
    <meta name="description" content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque aliquet nunc a elementum euismod. Interdum et malesuada fames ac ante ipsum primis in faucibus.">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/assets/css/0.styles.1f1142dd.css" as="style"><link rel="preload" href="/assets/js/app.54e24dc4.js" as="script"><link rel="preload" href="/assets/js/3.12cdb4dc.js" as="script"><link rel="preload" href="/assets/js/6.a4957363.js" as="script"><link rel="prefetch" href="/assets/js/10.c810b518.js"><link rel="prefetch" href="/assets/js/11.616c26eb.js"><link rel="prefetch" href="/assets/js/12.3d11e13f.js"><link rel="prefetch" href="/assets/js/13.80fc7817.js"><link rel="prefetch" href="/assets/js/14.7f5980b6.js"><link rel="prefetch" href="/assets/js/15.81df7efb.js"><link rel="prefetch" href="/assets/js/16.dc7eb4d8.js"><link rel="prefetch" href="/assets/js/17.fa1b58d3.js"><link rel="prefetch" href="/assets/js/18.1a79fdf5.js"><link rel="prefetch" href="/assets/js/19.deb3f6ba.js"><link rel="prefetch" href="/assets/js/2.3e04dc5f.js"><link rel="prefetch" href="/assets/js/20.db6df5b7.js"><link rel="prefetch" href="/assets/js/21.5e245fa2.js"><link rel="prefetch" href="/assets/js/22.b168b163.js"><link rel="prefetch" href="/assets/js/23.7bd29250.js"><link rel="prefetch" href="/assets/js/4.98e26cc5.js"><link rel="prefetch" href="/assets/js/5.163e8f83.js"><link rel="prefetch" href="/assets/js/7.8686dcb9.js"><link rel="prefetch" href="/assets/js/8.4305e23b.js"><link rel="prefetch" href="/assets/js/9.9da839ba.js">
    <link rel="stylesheet" href="/assets/css/0.styles.1f1142dd.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Journal</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/projects/interview-bot/" class="nav-link">
  Interview My Bot
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Webcam Eye Tracking with Neural Networks</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/projects/eyetracking/#summary" class="sidebar-link">Summary</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/eyetracking/#experiement" class="sidebar-link">Experiement</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/eyetracking/#current-techniques" class="sidebar-link">Current Techniques</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#theory" class="sidebar-link">Theory</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#results" class="sidebar-link">Results</a></li></ul></li><li><a href="/projects/eyetracking/#pipeline" class="sidebar-link">Pipeline</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/eyetracking/#eye-detection-and-tracking" class="sidebar-link">Eye Detection and Tracking</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#visualization" class="sidebar-link">Visualization</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#recording" class="sidebar-link">Recording</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#preprocessing" class="sidebar-link">Preprocessing</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#network-architecture" class="sidebar-link">Network Architecture</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#training" class="sidebar-link">Training</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#playback" class="sidebar-link">Playback</a></li></ul></li><li><a href="/projects/eyetracking/#safety-and-ethical-considerations" class="sidebar-link">Safety and Ethical Considerations</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/projects/eyetracking/#future-plans" class="sidebar-link">Future Plans</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/eyetracking/#ir-webcams" class="sidebar-link">IR Webcams</a></li><li class="sidebar-sub-header"><a href="/projects/eyetracking/#distribution-with-pretrained-network" class="sidebar-link">Distribution with Pretrained Network</a></li></ul></li><li><a href="/projects/eyetracking/#citations" class="sidebar-link">Citations</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="webcam-eye-tracking-with-neural-networks"><a href="#webcam-eye-tracking-with-neural-networks" class="header-anchor">#</a> Webcam Eye Tracking with Neural Networks</h1> <div style="position:relative;width:100%;height:0;padding-bottom:56.25%;margin-bottom:25px;"><iframe src="https://www.youtube.com/embed/L_LUpnjgPso" allowfullscreen="allowfullscreen" style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe></div> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <p>This project log documents building an eyetracking system using neural networks with raw webcam frames. The project trains a neural network with two pretrained ResNet branches. The first network receives frames from the webcam while the second is trained on a calibration image, a composite of the user looking at different locations during calibration time. The results show that it is possible to use a neural network to perform gaze tracking without specialized hardware or complex code.</p> <h2 id="experiement"><a href="#experiement" class="header-anchor">#</a> Experiement</h2> <h3 id="current-techniques"><a href="#current-techniques" class="header-anchor">#</a> Current Techniques</h3> <p>Current non-invasive eyetracking techniques fall under video oculography category, which uses video of the eyes and a light source as a reference point. There are five key techniques that are commonly used for eye tracking: 2D regression, 3D model, cross-ratio, appearance based and shape based. Each have their own pros and cons and are further discussed in [Kar, A., 2017].</p> <blockquote><p>Most video-based eye trackers work by illuminating the eye with an infrared light source. This light produces a
glint on the cornea of the eye and is called as corneal reflection. In most of the existing work, glint has been
used as the reference point for gaze estimation. [Chennamma, H., 2013]</p></blockquote> <blockquote><p>The steps commonly involved in passive video based eye tracking include user calibration, capturing video frames of the face and eye regions of user, eye detection and mapping with gaze coordinates on screen. The common methodology (called Pupil Center Corneal Reflection or PCCR method) involves using NIR LEDs to produce glints on the eye cornea surface and then capturing images/videos of the eye region. Gaze is estimated from the relative movement between the pupil center and glint positions. Webcams are mostly used; those operate at 30/60 fps frame rate and have infrared (IR) transmission filters to block out the visible light. [Kar, A., 2017]</p></blockquote> <h3 id="theory"><a href="#theory" class="header-anchor">#</a> Theory</h3> <p>The central hypothesis of this project was that it could be possible to build a neural network to learn to predict the gaze of a user. Unlike the above techniques, this method relies on a regular color unmodified webcam in an environment with average lighting conditions. There is no code written to determine angles, ratios or other variables. Instead, a network is provided with two images: a calibration composite image showing the user looking at calibration points and the current frame from the webcam is is trained to predict the location that of the gaze.</p> <h3 id="results"><a href="#results" class="header-anchor">#</a> Results</h3> <p>The results have been surprisingly effective. While it is clear that the current technique can't be used in a production like environment, the trained model is able to predict the general location of the gaze and follow it to some extent. Considering that this is done with a very small amounts of data by leveraging transfer learning, it may be worth investigating developing this further.</p> <h2 id="pipeline"><a href="#pipeline" class="header-anchor">#</a> Pipeline</h2> <p><img src="/assets/img/architecture.ad2ca5af.png" alt=""></p> <h3 id="eye-detection-and-tracking"><a href="#eye-detection-and-tracking" class="header-anchor">#</a> Eye Detection and Tracking</h3> <p>In order to do any work with gaze tracking, the eyes must first be extracted from the webcam frame. This ultimately ended up being the most time consuming part of the experiment, as all other steps rely on having an image of the eyes. OpenCV was chosen for the core of this step as it has functionality not only read from the webcam but also to show the image as well as some detection features discussed below.</p> <h4 id="eye-tracking"><a href="#eye-tracking" class="header-anchor">#</a> Eye Tracking</h4> <p>In order for the trained network to predict the location of the gaze, it must be presented with an image of the users eyes. While it is possible to send the whole frame from the camera to the network, this would result in poor performance, both during training time as the network would have to learn to discard most of the image and at runtime as the network would have to process a high resolution image. Instead, the first steps is to extract the eye region from the webcam frame. This is done in two steps: first, the face is detected, then the eyes are detected and extracted from the frame.</p> <h4 id="first-attempt-haar-cascades"><a href="#first-attempt-haar-cascades" class="header-anchor">#</a> First Attempt: Haar Cascades</h4> <p>OpenCV has <a href="https://github.com/opencv/opencv/tree/master/data/haarcascades" target="_blank" rel="noopener noreferrer">built in support for Haar Cascades<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, which includes support for face detection as well as eye detection. While the Haar Cascades provide reasonable detection, they also have some issues; generally they are less accurate and tend to jitter a lot from frame to frame resulting in the output image appearing to shake. It is possible to stabilize the jitter using several optimization tricks, such as blob detection of each iris and using a SavGol filter to stabilize the detections over time. In this experiment, it was found that these stabilization techniques added some value but at the expense of code complexity and high computational resources.</p> <h4 id="second-attempt-dnn-face-detection-face-region-selection"><a href="#second-attempt-dnn-face-detection-face-region-selection" class="header-anchor">#</a> Second Attempt: DNN Face Detection + Face Region Selection</h4> <p>Instead, a different approach was taken - OpenCV also has <a href="https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV" target="_blank" rel="noopener noreferrer">built in support for a DNN based face detector<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, which works much better than the equivalent Haar Cascade based face detector and is much more stable from frame to frame. Unfortunately, there is no DNN based eye detector, so the rules of basic human proportions were employed to extract the eyes from the face frame. To do this, the face frame is split into four equal parts (quarters), with the eyes being in the center of the second quarter. Then, one third of the frame is extracted centered on the eye line (one quarter extraction appeared to be sufficient for some subjects, but not all due to varying head shapes). The resulting frame is the eye region which can then be used in the next steps of the experiment.</p> <h4 id="image-enhancement-with-adaptive-brightness"><a href="#image-enhancement-with-adaptive-brightness" class="header-anchor">#</a> Image Enhancement with Adaptive Brightness</h4> <p>Eyetracking software typically uses infrared lights and camera to capture eye tracking because webcam quality and light conditions can result in varying image quality. Consumer webcams are very sensitive to light condition and for this experiment the recordings were done in a reasonably lit room and dark conditions were avoided. Still, this is not enough on its own so an additional step was employed: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html#clahe-contrast-limited-adaptive-histogram-equalization" target="_blank" rel="noopener noreferrer">Contrast Limited Adaptive Histogram Equalization (CLAHE)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> using OpenCV. Adaptive brightness cleans up the image so that all parts of it are equally lit up without loosing information. From OpenCV documentation:</p> <blockquote><p>So to solve this problem, adaptive histogram equalization is used. In this, image is divided into small blocks called “tiles” (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied.</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>def adaptive_brightness(img, clipLimit=1.0):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(8,8))
    lab[...,0] = clahe.apply(lab[...,0])
    final = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
    return final
</code></pre></div><h4 id="final-result"><a href="#final-result" class="header-anchor">#</a> Final Result</h4> <p><img src="/assets/img/eye-region.d721612a.png" alt=""></p> <h3 id="visualization"><a href="#visualization" class="header-anchor">#</a> Visualization</h3> <p>Stimulus presentation and visualization of predictions is done via a web app using SocketIO. The recording and playback application starts an eventlet server which serves a simple VueJS application that has a canvas and can receive four events: start training, start calibration, start demo and show point at x, y coordinates. The demo mode has three moving targets and one randomly jumping target which can be used as focus points to test a trained network.
<img src="/assets/img/web-app.5753cba3.png" alt=""></p> <h3 id="recording"><a href="#recording" class="header-anchor">#</a> Recording</h3> <h4 id="multicamera-and-capture-angles"><a href="#multicamera-and-capture-angles" class="header-anchor">#</a> Multicamera and Capture Angles</h4> <p>It is possible to use OpenCV to read from multiple cameras at the same time and the code implements this so that a single recording session can be recorded from multiple angles and with different cameras, such as the built in camera on the laptop along with several other cameras. Although the code implements multiple camera recording, a single <a href="https://www.amazon.com/Logitech-C920S-Webcam-Privacy-Shutter/dp/B07K95WFWM" target="_blank" rel="noopener noreferrer">Logitech C920S<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> was used to keep samples consistent. The camera was placed at the four cardinal locations around the screen as illustrated in this composite image.</p> <p><img src="/assets/img/composite.337dacb8.png" alt=""></p> <h4 id="capture-points"><a href="#capture-points" class="header-anchor">#</a> Capture Points</h4> <p>Each recording has 50 points which are presented to the user for training. This includes 9 calibration points, 16 evenly distributed fixed locations and 25 randomly generated points. At present, the calibration points are arbitrarily determined to approximate represent what might be ideal positions, however further investigation in research is necessary to determine if there is a best recommended layout. The fixed locations are determined by subdividing the screen into 1/4th size chunks horizontally and vertically - again, further research is required to determine if there are better locations to choose from. Note that the fixed locations do not repeat the points included in the calibration set - this is because the calibration points are also used as training points during training so the missing 9 points go towards the random points generation.</p> <p><img src="/assets/img/recording-points.22533182.png" alt=""></p> <h3 id="preprocessing"><a href="#preprocessing" class="header-anchor">#</a> Preprocessing</h3> <p>The preprocessing step has two components: first, for each recording the script collects the calibration point frames from the recording folder and creates a stitched composite of them. This image is the calibration image, used at the reference point for training time (at playback time, the user creates a calibration before a demo). Secondly, the script generates a labels.csv from the folder structure and file names. The resulting steps are illustrated below.</p> <p><img src="/assets/img/dataframe.6fd7936b.png" alt=""> <img src="/assets/img/calibration.69675efa.png" alt=""></p> <h3 id="network-architecture"><a href="#network-architecture" class="header-anchor">#</a> Network Architecture</h3> <p>The network architecture chosen for this project is being called a YNet. The YNet has two branches, one for the calibration image and the other for the current frame, and a head that concatenates the activations from both branches into one output. Both branches have pretrained ResNet18 cut just before their output layer so that the raw activations can be passed forward. XResNets, a variant with modifications from the Bag of Tricks paper as implemented by fast.ai were also tested but they performed poorly compared to a pretrained ResNet. As a side note, the network was implemented as a &quot;generic YNet&quot; (that is to say the branches are called &quot;left&quot; and &quot;right&quot;) because this network architecture may be useful in many other areas, such as robotics stereo vision with each branch handling one of two cameras eye width apart.</p> <p><img src="/assets/img/network.28446d3d.png" alt=""></p> <div class="language-YNet extra-class"><pre class="language-text"><code>class Branch(nn.Module):
	def __init__(self):
		super(Branch, self).__init__()
		self.body = create_body(models.resnet18)
		self.head = create_head(1024, 512)[:-4]
	
	def forward(self, x):
		x = self.body(x)
		x = self.head(x)
		return x
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>class Head(nn.Module):
	def __init__(self):
		super(Head, self).__init__()
		self.head = nn.Sequential(
				nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
				nn.Dropout(p=0.5, inplace=False),
				nn.Linear(in_features=1024, out_features=512, bias=True),
				nn.LeakyReLU(inplace=True),
			
				nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
				nn.Dropout(p=0.5, inplace=False),
				nn.Linear(in_features=512, out_features=2, bias=True),
				nn.Sigmoid())
    
    def forward(self, x, y):
		z = torch.cat([x, y], dim=1)
		z = self.head(z)
		return z
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>class YNetTrain(nn.Module):
	def __init__(self):
		super(YNetTrain, self).__init__()
		self.left = Branch()
		self.right = Branch()
		self.head = Head()
			
	@staticmethod
	def split_layers(m):
		groups = [[m.left.body, m.right.body]]
		groups += [[m.left.head, m.right.head]]
		groups += [[m.head]]
		return groups	
			
	def forward(self, x, y):
		x = self.left(x)
		y = self.left(y)
		z = self.head(x, y)
		return z
</code></pre></div><h3 id="training"><a href="#training" class="header-anchor">#</a> Training</h3> <p>Training is done using the standard fast.ai training loop using One Cycle training. First the pretrained ResNets are frozen and only the head is trained for a few epochs. Next, using fast.ai's learning rate finder, an optimal learning rate is determined with the ResNets unfrozen. The new discriminative learning rates are applied to the network and trained for several more epochs. The results from the last few epochs look like this:</p> <p><img src="/images/1920x1080.png" alt=""></p> <h3 id="playback"><a href="#playback" class="header-anchor">#</a> Playback</h3> <h4 id="calibration-network-activation-caching"><a href="#calibration-network-activation-caching" class="header-anchor">#</a> Calibration Network Activation Caching</h4> <p>After the network is trained, the trained weights are imported into the Python application and loaded into an instantiated network using PyTorch's <code>load_state_dict</code>. However, the trained network is not used directly. The trained network is passed to another network which splits the left and right branches. The left branch, used for calibration is not used during the forward pass - instead, the network has a calibrate function which takes in a calibration image. This calibration image is passed to the calibrationBranch and the activations from the branch are cached. Then when the network is used to make a prediction, the cached activation is passed to the network along with the current frame. This is done because the calibration image does not change at run time and while passing it and the frame image to the network is possible, it uses compute resources unnecessarily. By caching the calibration output, the whole left branch is removed from the forward pass, cutting the compute time in half. Currently the demo loop with frame grab, image conversion, prediction and dispatch to display runs at about 100 fps.</p> <div class="language- extra-class"><pre class="language-text"><code>class YNetPlayback(nn.Module):
	def __init__(self, trainedYNet):
		super(YNetPlayback, self).__init__()
		self.calibrationBranch = trainedYNet.left
		self.predictionBranch = trainedYNet.right
		self.head = trainedYNet.head
	
	def calibrate(self, calibrationImage):
		self.calibration = self.calibrationBranch(calibrationImage)
		
	def forward(self, y):
		z = self.head(self.calibration, self.predictionBranch(y))
		return z
</code></pre></div><h4 id="smoothing-with-savitzky-golay-filter"><a href="#smoothing-with-savitzky-golay-filter" class="header-anchor">#</a> Smoothing with Savitzky-Golay Filter</h4> <p>Finally, the last step involved is cleaning up and stablizing the output. Even though the network is trained well, it's predictions from frame to frame can vary wildly because it does not have any sense of change over time (i.e. an RNN). The change between two frames represent two entirely different states, which will have activations different enough to cause significant jitter in the raw output. However, on average the predictions are reliable. While This issue is still an open question for improvement, it was side stepped by using a Savitzky-Golay Filter. As illustrated below, the parameters used for the filter remove the high frequency noise between frames but keeps the signal from large intentional movements. The final result is what appears to be smooth tracking of the eyes.</p> <p><img src="/images/1920x1080.png" alt=""></p> <h2 id="safety-and-ethical-considerations"><a href="#safety-and-ethical-considerations" class="header-anchor">#</a> Safety and Ethical Considerations</h2> <p>One major issue was discovered during the development process: while the eye detection and tracking worked perfectly for the researcher and a test subject of Caucasian decent, it completely failed to detect and track the eyes of a subject of Asian decent because of difference in facial structure. This was corrected by adjusting the size of the eye region to properly detect the eyes. Even with this change, the sample group of this project is too small and not diverse enough to represent a general population. Further testing is required before this eyetracking can be deployed to the general public.</p> <h2 id="future-plans"><a href="#future-plans" class="header-anchor">#</a> Future Plans</h2> <h3 id="ir-webcams"><a href="#ir-webcams" class="header-anchor">#</a> IR Webcams</h3> <p>As noted above, eyetracking systems typically employ IR based camera systems because they are more resilient to lighting conditions. The webcams typically have an IR filter which only allows infrared light through and an infrared led to light the scene. Human irises are reflect a significant amount of IR light so this system works well together. Unlike those systems however, this project uses a plain color webcam with no additional lighting which affects it's quality. It is possible to modify existing webcams to enhance their infrared receptiveness and to add IR LEDs. This is being considered for future work to improve the performance of the project.</p> <h3 id="distribution-with-pretrained-network"><a href="#distribution-with-pretrained-network" class="header-anchor">#</a> Distribution with Pretrained Network</h3> <p>The final design on this project is intended to leverage transfer learning. The goal is to implement the code to train a final prediction model on a diverse group of many people to capture a good eyetracking model which is distributed along with the code. When a user wants to adopt the eyetracking program, they first provide a few training samples. Using discriminative learning rate, the application fine tunes the model with training and model freezing and unfreezing so that the network is uniquely tailored towards that specific user.</p> <h2 id="citations"><a href="#citations" class="header-anchor">#</a> Citations</h2> <p>[1] Kar, A., &amp; Corcoran, P. (2017). A review and analysis of eye-gaze estimation systems, algorithms and performance evaluation methods in consumer platforms. IEEE Access, 5, 16495-16519. Retrieved from https://ieeexplore.ieee.org/abstract/document/8003267 and https://arxiv.org/pdf/1708.01817.pdf</p> <p>[2] Chennamma, H. R., &amp; Yuan, X. (2013). A survey on eye-gaze tracking techniques. arXiv preprint arXiv:1312.6410. Retrieved from https://arxiv.org/abs/1312.6410</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.54e24dc4.js" defer></script><script src="/assets/js/3.12cdb4dc.js" defer></script><script src="/assets/js/6.a4957363.js" defer></script>
  </body>
</html>
