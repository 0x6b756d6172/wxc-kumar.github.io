(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{224:function(e,t,a){e.exports=a.p+"assets/img/architecture.ad2ca5af.png"},225:function(e,t,a){e.exports=a.p+"assets/img/eye-region.d721612a.png"},226:function(e,t,a){e.exports=a.p+"assets/img/web-app.5753cba3.png"},227:function(e,t,a){e.exports=a.p+"assets/img/composite.337dacb8.png"},228:function(e,t,a){e.exports=a.p+"assets/img/recording-points.22533182.png"},229:function(e,t,a){e.exports=a.p+"assets/img/dataframe.6fd7936b.png"},230:function(e,t,a){e.exports=a.p+"assets/img/calibration.69675efa.png"},231:function(e,t,a){e.exports=a.p+"assets/img/network.28446d3d.png"},232:function(e,t,a){e.exports=a.p+"assets/img/training.52f66d96.png"},253:function(e,t,a){"use strict";a.r(t);var i=a(0),r=Object(i.a)({},(function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[i("h1",{attrs:{id:"webcam-gaze-tracking-with-neural-networks"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#webcam-gaze-tracking-with-neural-networks"}},[e._v("#")]),e._v(" Webcam Gaze Tracking with Neural Networks")]),e._v(" "),i("Hero"),e._v(" "),i("h2",{attrs:{id:"summary"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),i("p",[e._v("This project log documents building an gaze tracking system using neural networks with raw webcam frames. The project trains a neural network with two pretrained ResNet branches. The first network receives frames from the webcam while the second is trained on a calibration image, a composite of the user looking at different locations during calibration time. The results show that it is possible to use a neural network to perform gaze tracking without specialized hardware or complex code.")]),e._v(" "),i("h2",{attrs:{id:"experiment"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#experiment"}},[e._v("#")]),e._v(" Experiment")]),e._v(" "),i("h3",{attrs:{id:"current-techniques"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#current-techniques"}},[e._v("#")]),e._v(" Current Techniques")]),e._v(" "),i("p",[e._v("Current non-invasive gaze tracking techniques fall under video oculography category, which uses video of the eyes and a light source as a reference point. There are five key techniques that are commonly used for eye tracking: 2D regression, 3D model, cross-ratio, appearance based and shape based. Each have their own pros and cons and are further discussed in [Kar, A., 2017].")]),e._v(" "),i("blockquote",[i("p",[e._v("Most video-based eye trackers work by illuminating the eye with an infrared light source. This light produces a\nglint on the cornea of the eye and is called as corneal reflection. In most of the existing work, glint has been\nused as the reference point for gaze estimation. [Chennamma, H., 2013]")])]),e._v(" "),i("blockquote",[i("p",[e._v("The steps commonly involved in passive video based eye tracking include user calibration, capturing video frames of the face and eye regions of user, eye detection and mapping with gaze coordinates on screen. The common methodology (called Pupil Center Corneal Reflection or PCCR method) involves using NIR LEDs to produce glints on the eye cornea surface and then capturing images/videos of the eye region. Gaze is estimated from the relative movement between the pupil center and glint positions. Webcams are mostly used; those operate at 30/60 fps frame rate and have infrared (IR) transmission filters to block out the visible light. [Kar, A., 2017]")])]),e._v(" "),i("h3",{attrs:{id:"theory"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#theory"}},[e._v("#")]),e._v(" Theory")]),e._v(" "),i("p",[e._v("The central hypothesis of this project was that it could be possible to build a neural network to learn to predict the gaze of a user. Unlike the above techniques, this method relies on a regular color unmodified webcam in an environment with average lighting conditions. There is no code written to determine angles, ratios or other variables. Instead, a network is provided with two images: a calibration composite image showing the user looking at calibration points and the current frame from the webcam is is trained to predict the location that of the gaze.")]),e._v(" "),i("h3",{attrs:{id:"results"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#results"}},[e._v("#")]),e._v(" Results")]),e._v(" "),i("p",[i("strong",[e._v("This project is still a work in progress")]),e._v(", however, the results are encouraging. While it is clear that the current technique can't be used in a production like environment, the trained model is able to predict the general location of the gaze and follow it to some extent. The results suggest that if this process was converted from a regression problem to a classification problem such that the network predicts which region of a sub divided screen, it would perform well. Considering that this is done with a very small amounts of data by leveraging transfer learning without an IR camera, it may be worth investigating developing this further.")]),e._v(" "),i("h2",{attrs:{id:"pipeline"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#pipeline"}},[e._v("#")]),e._v(" Pipeline")]),e._v(" "),i("p",[i("img",{attrs:{src:a(224),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"eye-detection-and-tracking"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#eye-detection-and-tracking"}},[e._v("#")]),e._v(" Eye Detection and Tracking")]),e._v(" "),i("p",[e._v("In order to do any work with gaze tracking, the eyes must first be extracted from the webcam frame. This ultimately ended up being the most time consuming part of the experiment, as all other steps rely on having an image of the eyes. OpenCV was chosen for the core of this step as it has functionality not only read from the webcam but also to show the image as well as some detection features discussed below.")]),e._v(" "),i("h4",{attrs:{id:"eye-tracking"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#eye-tracking"}},[e._v("#")]),e._v(" Eye Tracking")]),e._v(" "),i("p",[e._v("In order for the trained network to predict the location of the gaze, it must be presented with an image of the users eyes. While it is possible to send the whole frame from the camera to the network, this would result in poor performance, both during training time as the network would have to learn to discard most of the image and at runtime as the network would have to process a high resolution image. Instead, the first steps is to extract the eye region from the webcam frame. This is done in two steps: first, the face is detected, then the eyes are detected and extracted from the frame.")]),e._v(" "),i("h4",{attrs:{id:"first-attempt-haar-cascades"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#first-attempt-haar-cascades"}},[e._v("#")]),e._v(" First Attempt: Haar Cascades")]),e._v(" "),i("p",[e._v("OpenCV has "),i("a",{attrs:{href:"https://github.com/opencv/opencv/tree/master/data/haarcascades",target:"_blank",rel:"noopener noreferrer"}},[e._v("built in support for Haar Cascades"),i("OutboundLink")],1),e._v(", which includes support for face detection as well as eye detection. While the Haar Cascades provide reasonable detection, they also have some issues; generally they are less accurate and tend to jitter a lot from frame to frame resulting in the output image appearing to shake. It is possible to stabilize the jitter using several optimization tricks, such as blob detection of each iris and using a SavGol filter to stabilize the detections over time. In this experiment, it was found that these stabilization techniques added some value but at the expense of code complexity and high computational resources.")]),e._v(" "),i("h4",{attrs:{id:"second-attempt-dnn-face-detection-face-region-selection"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#second-attempt-dnn-face-detection-face-region-selection"}},[e._v("#")]),e._v(" Second Attempt: DNN Face Detection + Face Region Selection")]),e._v(" "),i("p",[e._v("Instead, a different approach was taken - OpenCV also has "),i("a",{attrs:{href:"https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV",target:"_blank",rel:"noopener noreferrer"}},[e._v("built in support for a DNN based face detector"),i("OutboundLink")],1),e._v(", which works much better than the equivalent Haar Cascade based face detector and is much more stable from frame to frame. Unfortunately, there is no DNN based eye detector, so the rules of basic human proportions were employed to extract the eyes from the face frame. To do this, the face frame is split into four equal parts (quarters), with the eyes being in the center of the second quarter. Then, one third of the frame is extracted centered on the eye line (one quarter extraction appeared to be sufficient for some subjects, but not all due to varying head shapes). The resulting frame is the eye region which can then be used in the next steps of the experiment.")]),e._v(" "),i("h4",{attrs:{id:"image-enhancement-with-adaptive-brightness"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#image-enhancement-with-adaptive-brightness"}},[e._v("#")]),e._v(" Image Enhancement with Adaptive Brightness")]),e._v(" "),i("p",[e._v("gaze tracking software typically uses infrared lights and camera to capture eye tracking because webcam quality and light conditions can result in varying image quality. Consumer webcams are very sensitive to light condition and for this experiment the recordings were done in a reasonably lit room and dark conditions were avoided. Still, this is not enough on its own so an additional step was employed: "),i("a",{attrs:{href:"https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html#clahe-contrast-limited-adaptive-histogram-equalization",target:"_blank",rel:"noopener noreferrer"}},[e._v("Contrast Limited Adaptive Histogram Equalization (CLAHE)"),i("OutboundLink")],1),e._v(" using OpenCV. Adaptive brightness cleans up the image so that all parts of it are equally lit up without loosing information. From OpenCV documentation:")]),e._v(" "),i("blockquote",[i("p",[e._v("So to solve this problem, adaptive histogram equalization is used. In this, image is divided into small blocks called “tiles” (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied.")])]),e._v(" "),i("div",{staticClass:"language- extra-class"},[i("pre",{pre:!0,attrs:{class:"language-text"}},[i("code",[e._v("def adaptive_brightness(img, clipLimit=1.0):\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(8,8))\n    lab[...,0] = clahe.apply(lab[...,0])\n    final = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    return final\n")])])]),i("h4",{attrs:{id:"final-result"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#final-result"}},[e._v("#")]),e._v(" Final Result")]),e._v(" "),i("p",[i("img",{attrs:{src:a(225),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"visualization"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#visualization"}},[e._v("#")]),e._v(" Visualization")]),e._v(" "),i("p",[e._v("Stimulus presentation and visualization of predictions is done via a web app using SocketIO. The recording and playback application starts an eventlet server which serves a simple VueJS application that has a canvas and can receive four events: start training, start calibration, start demo and show point at x, y coordinates. The demo mode has three moving targets and one randomly jumping target which can be used as focus points to test a trained network.\n"),i("img",{attrs:{src:a(226),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"recording"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#recording"}},[e._v("#")]),e._v(" Recording")]),e._v(" "),i("h4",{attrs:{id:"multicamera-and-capture-angles"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#multicamera-and-capture-angles"}},[e._v("#")]),e._v(" Multicamera and Capture Angles")]),e._v(" "),i("p",[e._v("It is possible to use OpenCV to read from multiple cameras at the same time and the code implements this so that a single recording session can be recorded from multiple angles and with different cameras, such as the built in camera on the laptop along with several other cameras. Although the code implements multiple camera recording, a single "),i("a",{attrs:{href:"https://www.amazon.com/Logitech-C920S-Webcam-Privacy-Shutter/dp/B07K95WFWM",target:"_blank",rel:"noopener noreferrer"}},[e._v("Logitech C920S"),i("OutboundLink")],1),e._v(" was used to keep samples consistent. The camera was placed at the four cardinal locations around the screen as illustrated in this composite image.")]),e._v(" "),i("p",[i("img",{attrs:{src:a(227),alt:""}})]),e._v(" "),i("h4",{attrs:{id:"capture-points"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#capture-points"}},[e._v("#")]),e._v(" Capture Points")]),e._v(" "),i("p",[e._v("Each recording has 50 points which are presented to the user for training. This includes 9 calibration points, 16 evenly distributed fixed locations and 25 randomly generated points. At present, the calibration points are arbitrarily determined to approximate represent what might be ideal positions, however further investigation in research is necessary to determine if there is a best recommended layout. The fixed locations are determined by subdividing the screen into 1/4th size chunks horizontally and vertically - again, further research is required to determine if there are better locations to choose from. Note that the fixed locations do not repeat the points included in the calibration set - this is because the calibration points are also used as training points during training so the missing 9 points go towards the random points generation.")]),e._v(" "),i("p",[i("img",{attrs:{src:a(228),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"preprocessing"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#preprocessing"}},[e._v("#")]),e._v(" Preprocessing")]),e._v(" "),i("p",[e._v("The preprocessing step has two components: first, for each recording the script collects the calibration point frames from the recording folder and creates a stitched composite of them. This image is the calibration image, used at the reference point for training time (at playback time, the user creates a calibration before a demo). Secondly, the script generates a labels.csv from the folder structure and file names. The resulting steps are illustrated below.")]),e._v(" "),i("p",[i("img",{attrs:{src:a(229),alt:""}}),e._v(" "),i("img",{attrs:{src:a(230),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"network-architecture"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#network-architecture"}},[e._v("#")]),e._v(" Network Architecture")]),e._v(" "),i("p",[e._v("The network architecture chosen for this project is being called a YNet. The YNet has two branches, one for the calibration image and the other for the current frame, and a head that concatenates the activations from both branches into one output. Both branches have pretrained ResNet18 cut just before their output layer so that the raw activations can be passed forward. Additional, all non-pretrained parts of the network were initialized with "),i("code",[e._v("nn.init.kaiming_normal")]),e._v('. XResNets, a variant with modifications from the Bag of Tricks paper as implemented by fast.ai were also tested but they performed poorly compared to a pretrained ResNet. As a side note, the network was implemented as a "generic YNet" (that is to say the branches are called "left" and "right") because this network architecture may be useful in many other areas, such as robotics stereo vision with each branch handling one of two cameras eye width apart.')]),e._v(" "),i("p",[i("img",{attrs:{src:a(231),alt:""}})]),e._v(" "),i("div",{staticClass:"language- extra-class"},[i("pre",{pre:!0,attrs:{class:"language-text"}},[i("code",[e._v("class Branch(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Branch, self).__init__()\n\t\tself.body = create_body(models.resnet18)\n\t\tself.head = create_head(1024, 512)[:-4]\n\t\n\tdef forward(self, x):\n\t\tx = self.body(x)\n\t\tx = self.head(x)\n\t\treturn x\n")])])]),i("div",{staticClass:"language- extra-class"},[i("pre",{pre:!0,attrs:{class:"language-text"}},[i("code",[e._v("class Head(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Head, self).__init__()\n\t\tself.head = nn.Sequential(\t\t\n\t\t\t\tnn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n\t\t\t\tnn.Dropout(p=0.5, inplace=False),\n\t\t\t\tnn.Linear(in_features=1024, out_features=512, bias=True),\n\t\t\t\tnn.LeakyReLU(inplace=True),\n\t\t\t\n\t\t\t\tnn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n\t\t\t\tnn.Dropout(p=0.5, inplace=False),\n\t\t\t\tnn.Linear(in_features=512, out_features=2, bias=True),\n\t\t\t\tnn.Sigmoid())\n\t\t\t\n\tdef forward(self, x, y):\n\t\tz = torch.cat([x, y], dim=1)\n\t\tz = self.head(z)\n\t\treturn z\n")])])]),i("div",{staticClass:"language- extra-class"},[i("pre",{pre:!0,attrs:{class:"language-text"}},[i("code",[e._v("class YNetTrain(nn.Module):\n\tdef __init__(self):\n\t\tsuper(YNetTrain, self).__init__()\n\t\tself.left = Branch()\n\t\tself.right = Branch()\n\t\tself.head = Head()\n\t\t\t\n\t@staticmethod\n\tdef split_layers(m):\n\t\tgroups = [[m.left.body, m.right.body]]\n\t\tgroups += [[m.left.head, m.right.head, m.head]]\n\t\treturn groups\t\n\t\t\t\n\tdef forward(self, x, y):\n\t\tx = self.left(x)\n\t\ty = self.right(y)\n\t\tz = self.head(x, y)\n\t\treturn z\n")])])]),i("h3",{attrs:{id:"training"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#training"}},[e._v("#")]),e._v(" Training")]),e._v(" "),i("p",[e._v("Training is done using the standard fast.ai training loop using One Cycle training, using SmoothL1Loss (Huber Loss). The training appears to achieve maximum convergence at around 25 epochs of training at a universal learning rate of 3e-3 which ran for approximately 9.5 minutes with 1800 samples. The following shows the metrics from the last 5 epochs. "),i("strong",[e._v("Note:")]),e._v(" There is currently an open question on the fast.ai forums for this project regarding strange behavior regarding pretrained ResNets which can be seen here: "),i("a",{attrs:{href:"https://forums.fast.ai/t/training-results-breaking-basic-rule-of-fast-ai-course/64043",target:"_blank",rel:"noopener noreferrer"}},[e._v("Training Results Breaking Basic Rule of fast.ai Course"),i("OutboundLink")],1)]),e._v(" "),i("p",[i("img",{attrs:{src:a(232),alt:""}})]),e._v(" "),i("h3",{attrs:{id:"playback"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#playback"}},[e._v("#")]),e._v(" Playback")]),e._v(" "),i("h4",{attrs:{id:"calibration-network-activation-caching"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#calibration-network-activation-caching"}},[e._v("#")]),e._v(" Calibration Network Activation Caching")]),e._v(" "),i("p",[e._v("After the network is trained, the trained weights are imported into the Python application and loaded into an instantiated network using PyTorch's "),i("code",[e._v("load_state_dict")]),e._v(". However, the trained network is not used directly. The trained network is passed to another network which splits the left and right branches. The left branch, used for calibration is not used during the forward pass - instead, the network has a calibrate function which takes in a calibration image. This calibration image is passed to the calibrationBranch and the activations from the branch are cached. Then when the network is used to make a prediction, the cached activation is passed to the network along with the current frame. This is done because the calibration image does not change at run time and while passing it and the frame image to the network is possible, it uses compute resources unnecessarily. By caching the calibration output, the whole left branch is removed from the forward pass, cutting the compute time in half. Currently the demo loop with frame grab, image conversion, prediction and dispatch to display runs at about 100 fps.")]),e._v(" "),i("div",{staticClass:"language- extra-class"},[i("pre",{pre:!0,attrs:{class:"language-text"}},[i("code",[e._v("class YNetPlayback(nn.Module):\n\tdef __init__(self, trainedYNet):\n\t\tsuper(YNetPlayback, self).__init__()\n\t\tself.calibrationBranch = trainedYNet.left\n\t\tself.predictionBranch = trainedYNet.right\n\t\tself.head = trainedYNet.head\n\t\n\tdef calibrate(self, calibrationImage):\n\t\tself.calibration = self.calibrationBranch(calibrationImage)\n\t\t\n\tdef forward(self, y):\n\t\tx = self.calibration\n\t\ty = self.predictionBranch(y)\n\t\tz = self.head(x, y)\n\t\treturn z\n")])])]),i("h4",{attrs:{id:"smoothing-with-savitzky-golay-filter"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#smoothing-with-savitzky-golay-filter"}},[e._v("#")]),e._v(" Smoothing with Savitzky-Golay Filter")]),e._v(" "),i("p",[e._v("Finally, the last step involved is cleaning up and stablizing the output. Even though the network is trained well, it's predictions from frame to frame can vary wildly because it does not have any sense of change over time (i.e. an RNN). The change between two frames represent two entirely different states, which will have activations different enough to cause significant jitter in the raw output. However, on average the predictions are reliable. While This issue is still an open question for improvement, it was side stepped by using a Savitzky-Golay Filter. The parameters used for the filter remove the high frequency noise between frames but keeps the signal from large intentional movements. The final result is what appears to be smooth tracking of the eyes.")]),e._v(" "),i("h2",{attrs:{id:"safety-and-ethical-considerations"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#safety-and-ethical-considerations"}},[e._v("#")]),e._v(" Safety and Ethical Considerations")]),e._v(" "),i("p",[e._v("One major issue was discovered during the development process: while the eye detection and tracking worked perfectly for the researcher and a test subject of Caucasian decent, it completely failed to detect and track the eyes of a subject of Asian decent because of difference in facial structure. This was corrected by adjusting the size of the eye region to properly detect the eyes. Even with this change, the sample group of this project is too small and not diverse enough to represent a general population. Further testing is required before this gaze tracking can be deployed to the general public.")]),e._v(" "),i("h2",{attrs:{id:"future-plans"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#future-plans"}},[e._v("#")]),e._v(" Future Plans")]),e._v(" "),i("h3",{attrs:{id:"regression-to-classification"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#regression-to-classification"}},[e._v("#")]),e._v(" Regression to Classification")]),e._v(" "),i("p",[e._v("The overall findings of this experiment show that the network is able to predict the general region of the screen that the gaze is on but performs less well when required to predict an exact (x, y) coordinate. This suggests that it may possible to convert the architecture to subdivide the screen into rectangular regions and predict which region the gaze is on. With a high enough resolution of the grid, it could be possible to predict the gaze with more accuracy using this method rather than regression.")]),e._v(" "),i("h3",{attrs:{id:"ir-webcams"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#ir-webcams"}},[e._v("#")]),e._v(" IR Webcams")]),e._v(" "),i("p",[e._v("As noted above, gaze tracking systems typically employ IR based camera systems because they are more resilient to lighting conditions. The webcams typically have an IR filter which only allows infrared light through and an infrared led to light the scene. Human irises are reflect a significant amount of IR light so this system works well together. Unlike those systems however, this project uses a plain color webcam with no additional lighting which affects it's quality. It is possible to modify existing webcams to enhance their infrared receptiveness and to add IR LEDs. This is being considered for future work to improve the performance of the project.")]),e._v(" "),i("h3",{attrs:{id:"distribution-with-pretrained-network"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#distribution-with-pretrained-network"}},[e._v("#")]),e._v(" Distribution with Pretrained Network")]),e._v(" "),i("p",[e._v("The final design on this project is intended to leverage transfer learning. The goal is to implement the code to train a final prediction model on a diverse group of many people to capture a good gaze tracking model which is distributed along with the code. When a user wants to adopt the gaze tracking program, they first provide a few training samples. Using discriminative learning rate, the application fine tunes the model with training and model freezing and unfreezing so that the network is uniquely tailored towards that specific user.")]),e._v(" "),i("h2",{attrs:{id:"citations"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#citations"}},[e._v("#")]),e._v(" Citations")]),e._v(" "),i("p",[e._v("[1] Kar, A., & Corcoran, P. (2017). A review and analysis of eye-gaze estimation systems, algorithms and performance evaluation methods in consumer platforms. IEEE Access, 5, 16495-16519. Retrieved from https://ieeexplore.ieee.org/abstract/document/8003267 and https://arxiv.org/pdf/1708.01817.pdf")]),e._v(" "),i("p",[e._v("[2] Chennamma, H. R., & Yuan, X. (2013). A survey on eye-gaze tracking techniques. arXiv preprint arXiv:1312.6410. Retrieved from https://arxiv.org/abs/1312.6410")])],1)}),[],!1,null,null,null);t.default=r.exports}}]);