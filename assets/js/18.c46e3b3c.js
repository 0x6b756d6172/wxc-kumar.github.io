(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{254:function(e,t,a){"use strict";a.r(t);var i=a(0),n=Object(i.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("Hero"),e._v(" "),a("h2",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v('This article is a collection of lessons learned in building deep learning based tools, such as Control and Eyetracking. The article is primarily directed at those that are building "real world applications" using deep learning. Here real world means that you need to collect or create a specific dataset unique to your problem, you need to train a network specific to your problem and you need to deploy that network in an application so that it can be moved out of Jupyter.')]),e._v(" "),a("h2",{attrs:{id:"terminology"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#terminology"}},[e._v("#")]),e._v(" Terminology")]),e._v(" "),a("p",[e._v("This article assumes some definitions:")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("Recording Application")]),e._v(": code written to collect data for a specific problem. For example, a recording application could collect images from a camera.")]),e._v(" "),a("li",[a("strong",[e._v("Trained Network")]),e._v(": a neural network trained on recorded data to make some prediction. For example, classify if this image has a truck in it.")]),e._v(" "),a("li",[a("strong",[e._v("Playback Application")]),e._v(": code written to take a trained model and make predictions in a production like environment. For example, a playback application collects images from a camera, then uses a trained network to predict if there is a truck in them.")]),e._v(" "),a("li",[a("strong",[e._v("Pipeline")]),e._v(": the collection of tools, applications and scripts that are involved in a machine learning application. For example, the scripts, applications and notebooks required to watch a camera and determine if a truck has pulled up at the loading dock.")])]),e._v(" "),a("h2",{attrs:{id:"lessons"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lessons"}},[e._v("#")]),e._v(" Lessons")]),e._v(" "),a("h3",{attrs:{id:"_1-neural-networks-are-easy-engineering-is-hard"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-neural-networks-are-easy-engineering-is-hard"}},[e._v("#")]),e._v(" #1: Neural Networks are Easy, Engineering is Hard")]),e._v(" "),a("p",[e._v("The first lesson you will learn in ML Engineering is that deep learning tasks such as building a neural network are actually surprisingly easy. Thanks to libraries and tools like fast.ai, PyTorch, Pandas, Numpy, SciPy and the many others, the actual act of building and training a network often takes much less time than other parts of the pipeline.")]),e._v(" "),a("p",[e._v("Note, this is not to say that building a "),a("em",[e._v("good")]),e._v(" model is easy, only the tasks involved in building a model, such as constructing the network data structure, passing it data, training and evaluating is easy because libraries provide all the interfaces necessary to do this quickly. For example, thanks to fast.ai's, you will never need to write a state of the art training loop. With PyTorch's catalog of layers, you will never need to implement a convolution layer or a ReLU node. These components allow quick and easy construction of trained networks. You of course will still need to experiment with architectures and hyperparameters based on your use case.")]),e._v(" "),a("p",[e._v('In contrast, building an application to collect and prepare data and use the trained network "in production" will be where 80% of your time goes. This includes tasks like data preparation including exploration, clean up and transformation. This also includes things like deployment with the model behind a web service or integrated in your application. For example, if you are building realtime applications, being able to use a model for rapid predictions will be an added challenge due to their execution time of each forward pass.')]),e._v(" "),a("h3",{attrs:{id:"_2-build-the-end-to-end-pipeline-first"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-build-the-end-to-end-pipeline-first"}},[e._v("#")]),e._v(" #2: Build the End to End Pipeline First")]),e._v(" "),a("p",[e._v("Because engineering is hard, you should attempt to minimize the amount of time you put in any one step and focus on building the end to end pipeline first. Get data moving from recording, preprocess, training and playback as soon as possible before improving performance, trying new architectures or trying new techniques. In this scenario, the data doesn't need to be perfect or complete and the trained network doesn't even need to have achieved convergence. This step is solely about making sure the pieces are in place. This is for an important reason: your process of building and training this network and it's parent application are going to require experimentation. Sometimes this will require huge changes to your code. This is exponentially harder to do if you completely built out your end applications before ensuring that the data the application generates is correct and the network that is trained works in your applications.")]),e._v(" "),a("h3",{attrs:{id:"_3-don-t-underestimate-the-power-of-preprocessing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-don-t-underestimate-the-power-of-preprocessing"}},[e._v("#")]),e._v(" #3: Don't Underestimate the Power of Preprocessing")]),e._v(" "),a("p",[e._v('When building your recording application, it may be tempting to record "perfect" data - data that does not need to be preprocessed and can be fed directly into neural network training. Fight the urge to do this. A preprocessing step can save you huge amounts of time by allowing you to offload both cognitive and computing load into a once-in-a-while step. During experimentation, you will almost certainly have to change code somewhere in the recording application. If your recording application needs to run under some constraints like at 60 frames per second, you may need to spend a great deal of time optimizing recording code only to have to rewrite it after realizing that you need to change the way the data is structured. By comparison, a preprocess step doesn\'t need to be performant, it can be setup to run only once after code is changed. Store data as raw as possible from your recording application and use the preprocess step to enrich it for training.')]),e._v(" "),a("h3",{attrs:{id:"_4-use-jupyter-to-the-fullest-extent"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-use-jupyter-to-the-fullest-extent"}},[e._v("#")]),e._v(" #4: Use Jupyter to the Fullest Extent")]),e._v(" "),a("p",[e._v("Jupyter notebooks are traditionally used for data science related tasks, such as data exploration, graphing and small one off tasks. However, the core technology behind Jupyter lets you experiment with any Python code without having to build and run a full application. It's even possible to do full application and library development inside Jupyter, as fast.ai has done and even created the "),a("a",{attrs:{href:"https://www.fast.ai/2019/12/02/nbdev/",target:"_blank",rel:"noopener noreferrer"}},[e._v("nbdev library"),a("OutboundLink")],1),e._v(" for it. Whether you choose to build your whole application in Jupyter or not, use it to experiment and test out functions and classes and do micro benchmarks using the "),a("code",[e._v("%timeit")]),e._v(" directive.")]),e._v(" "),a("h3",{attrs:{id:"_5-read-research-papers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-read-research-papers"}},[e._v("#")]),e._v(" #5: Read Research Papers")]),e._v(" "),a("p",[e._v("You may think that because you are building an end user application that your traditional experience in software engineering, such as building Python applications or web based applications will suffice. However, the machine learning process is full of lots of small decisions that add up to be significant. Research papers related to the task you are doing will may give you an idea of which direction to go with your application design that you may otherwise have to discover yourself through experimentation.")]),e._v(" "),a("h3",{attrs:{id:"_6-don-t-sit-around-watching-tensorboard"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-don-t-sit-around-watching-tensorboard"}},[e._v("#")]),e._v(" #6: Don't Sit Around Watching Tensorboard")]),e._v(" "),a("p",[e._v("Your training process will output stats during training, such as training and validation loss over the training epochs, either through Jupyter, console or Tensorboard. It may be very tempting to sit and wait for the next epoch to complete to see what the performance looks like in the next step and then the next and the next. You will find yourself wasting a lot of time watching these stats change slowly. Fight the urge to watch the training and instead use your time in other engineering tasks. The final result will be there when it's done.")]),e._v(" "),a("h3",{attrs:{id:"_7-use-hyper-parameter-optimization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-use-hyper-parameter-optimization"}},[e._v("#")]),e._v(" #7: Use Hyper Parameter Optimization")]),e._v(" "),a("p",[e._v("The machine learning process is full of small variables that impact the quality of training and speed. These variables may express themselves differently in combinations with other variables and because running several epochs of a learning cycle can take a lot of time (hours to days, depending on the dataset), you will not be able to manually test each combination to find the best results and will only lose time doing so especially if your data format changes or your network architecture changes. Instead, use a hyper parameter optimization strategy such as Bayesian Optimization. "),a("a",{attrs:{href:"https://ax.dev/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ax"),a("OutboundLink")],1),e._v(" and "),a("a",{attrs:{href:"https://botorch.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("BoTorch"),a("OutboundLink")],1),e._v(" are excellent choices if you are working with PyTorch as they are designed to all work togther. There is also an excellent Python library "),a("a",{attrs:{href:"https://github.com/fmfn/BayesianOptimization",target:"_blank",rel:"noopener noreferrer"}},[e._v("bayesian-optimization"),a("OutboundLink")],1),e._v(".")])],1)}),[],!1,null,null,null);t.default=n.exports}}]);